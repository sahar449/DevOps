name: EKS Infrastructure Management

on:
  workflow_run:
    workflows: ["CI Build & Test"]
    types:
      - completed
  workflow_dispatch:
    inputs:
      action:
        description: "Choose action"
        required: true
        type: choice
        options:
          - apply
          - destroy

permissions:
  id-token: write
  contents: read

env:
  AWS_REGION: us-west-2
  CLUSTER_NAME: eksdemo-cluster
  VPC_NAME: eksdemo-vpc

jobs:
  # ============================================
  # Job 1: APPLY (Deploy)
  # ============================================
  apply:
    runs-on: ubuntu-latest
    if: |
      (github.event_name == 'workflow_dispatch' && github.event.inputs.action == 'apply') ||
      (github.event_name == 'workflow_run' && github.event.workflow_run.conclusion == 'success')
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Configure AWS
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          aws-region: ${{ env.AWS_REGION }}
          role-session-name: github-actions-apply

      - name: Update kubeconfig for EKS
        run: |
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.CLUSTER_NAME }}

      - name: Configure aws-auth
        run: |
          cat <<EOF | kubectl apply -f -
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: aws-auth
            namespace: kube-system
          data:
            mapUsers: |
              - userarn: arn:aws:iam::${{ secrets.AWS_ACCOUNT_NUMBER }}:user/${{ secrets.AWS_USER }}
                username: ${{ secrets.AWS_USER }}
                groups:
                  - system:masters
          EOF
      
      - name: Install Prometheus Operator CRDs
        run: |
          echo "ðŸ“¦ Installing Prometheus Operator CRDs..."

          kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.77.2/example/prometheus-operator-crd/monitoring.coreos.com_alertmanagerconfigs.yaml
          kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.77.2/example/prometheus-operator-crd/monitoring.coreos.com_alertmanagers.yaml
          kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.77.2/example/prometheus-operator-crd/monitoring.coreos.com_podmonitors.yaml
          kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.77.2/example/prometheus-operator-crd/monitoring.coreos.com_probes.yaml
          kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.77.2/example/prometheus-operator-crd/monitoring.coreos.com_prometheuses.yaml
          kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.77.2/example/prometheus-operator-crd/monitoring.coreos.com_prometheusrules.yaml
          kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.77.2/example/prometheus-operator-crd/monitoring.coreos.com_scrapeconfigs.yaml
          kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.77.2/example/prometheus-operator-crd/monitoring.coreos.com_servicemonitors.yaml
          kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.77.2/example/prometheus-operator-crd/monitoring.coreos.com_thanosrulers.yaml

          echo "â³ Waiting for CRDs to be established..."

          kubectl wait --for=condition=established crd/prometheuses.monitoring.coreos.com --timeout=120s
          kubectl wait --for=condition=established crd/alertmanagers.monitoring.coreos.com --timeout=120s
          kubectl wait --for=condition=established crd/prometheusrules.monitoring.coreos.com --timeout=120s

          echo "âœ… Prometheus CRDs are ready"
          
    
      - name: Install ArgoCD
        run: |
          echo "ðŸ“¦ Installing Prometheus Operator CRDs..."
          
          CRDS=(
            "https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.77.2/example/prometheus-operator-crd/monitoring.coreos.com_alertmanagerconfigs.yaml"
            "https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.77.2/example/prometheus-operator-crd/monitoring.coreos.com_alertmanagers.yaml"
            "https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.77.2/example/prometheus-operator-crd/monitoring.coreos.com_podmonitors.yaml"
            "https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.77.2/example/prometheus-operator-crd/monitoring.coreos.com_probes.yaml"
            "https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.77.2/example/prometheus-operator-crd/monitoring.coreos.com_prometheusagents.yaml"
            "https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.77.2/example/prometheus-operator-crd/monitoring.coreos.com_prometheuses.yaml"
            "https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.77.2/example/prometheus-operator-crd/monitoring.coreos.com_prometheusrules.yaml"
            "https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.77.2/example/prometheus-operator-crd/monitoring.coreos.com_scrapeconfigs.yaml"
            "https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.77.2/example/prometheus-operator-crd/monitoring.coreos.com_servicemonitors.yaml"
            "https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.77.2/example/prometheus-operator-crd/monitoring.coreos.com_thanosrulers.yaml"
          )
          
          for CRD_URL in "${CRDS[@]}"; do
            echo "Installing $(basename $CRD_URL)..."
            kubectl apply --server-side --force-conflicts -f "$CRD_URL"
          done
          
          echo "â³ Waiting for CRDs to be established..."
          kubectl wait --for condition=established --timeout=60s crd/prometheuses.monitoring.coreos.com
          kubectl wait --for condition=established --timeout=60s crd/alertmanagers.monitoring.coreos.com
          kubectl wait --for condition=established --timeout=60s crd/prometheusrules.monitoring.coreos.com
          echo "âœ… CRDs ready"

      - name: Install ArgoCD and Prometheus Monitoring using Helm
        run: |
          echo "ðŸš€ Installing Prometheus Stack..."
          helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
          helm repo update
          
          helm upgrade --install prom-stack prometheus-community/kube-prometheus-stack \
          --namespace monitoring \
          --create-namespace \
          --version 67.4.0 \
          --set grafana.enabled=true \
          --set grafana.adminPassword="${{ secrets.GRAFANA_PASS }}" \
          --set alertmanager.config.global.smtp_auth_password="${{ secrets.GOOGLE_SECRET_PROM }}" \
          --set grafana.service.type=LoadBalancer \
          --set grafana.service.port=80 \
          --set-string grafana.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-scheme"=internet-facing \
          --set grafana.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-name"="${{ env.GRAFANA_CLB }}" \
          --set prometheus.prometheusSpec.retention=30d

          echo "âœ… Prometheus Stack installed"
          
          echo ""
          echo "ðŸš€ Installing ArgoCD..."
          helm repo add argo https://argoproj.github.io/argo-helm
          helm repo update
          
          helm upgrade --install argocd argo/argo-cd \
            --namespace argocd \
            --create-namespace \
            --set server.service.type=LoadBalancer \
            --set-string server.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-scheme"=internet-facing \
            --set server.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-name"="argocd"
          
          echo "â³ Waiting for ArgoCD server rollout..."
          kubectl -n argocd rollout status deployment/argocd-server --timeout=5m
          
          ARGOCD_LB_DNS=$(kubectl -n argocd get svc argocd-server -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
          echo "ðŸŒ ArgoCD LoadBalancer DNS: $ARGOCD_LB_DNS"
          
          ARGOCD_ADMIN_PASSWORD=$(kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 --decode)
          echo "ðŸ”‘ ArgoCD initial admin password: $ARGOCD_ADMIN_PASSWORD"
          
          echo "âœ… ArgoCD installed"

      - name: Get VPCID, Secrets and Deploy Apps
        run: |
          export VPC_ID=$(aws ec2 describe-vpcs \
          --filters "Name=tag:Name,Values=${{ env.VPC_NAME }}" \
          --query "Vpcs[0].VpcId" --output text)
          export ECR_NAME=${{ secrets.ECR_NAME }}
          export AWS_ACCOUNT_NUMBER=${{ secrets.AWS_ACCOUNT_NUMBER }}
          export GOOGLE_SECRET_PROM=${{ secrets.GOOGLE_SECRET_PROM }}
          export GRAFANA_PASS=${{ secrets.GRAFANA_PASS }}
          
          echo "ðŸš€ Deploying ArgoCD Applications..."
          envsubst < ArgoCD/externalCharts/alb_dns.yaml | kubectl apply -f -
          envsubst < ArgoCD/externalCharts/secrets.yaml | kubectl apply -f -
          envsubst < ArgoCD/myChart/argo.yaml | kubectl apply -f -
          envsubst < ArgoCD/monitoring/argo.yaml | kubectl apply -f -
                      
          echo "â³ Waiting for applications to sync..."
          sleep 30
          
          echo ""
          echo "ðŸ“‹ ArgoCD Applications Status:"
          kubectl get applications -n argocd
          
          echo ""
          echo "ðŸ“Š Monitoring Pods:"
          kubectl get pods -n monitoring
          
          echo ""
          echo "============================================"
          echo "ðŸ“Š GRAFANA DASHBOARD"
          echo "============================================"
          GRAFANA_URL=$(kubectl get svc prom-stack-grafana -n monitoring \
            -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "Pending...")
          
          if [[ "$GRAFANA_URL" != "Pending..." ]]; then
            echo "âœ… URL: http://$GRAFANA_URL"
            echo "ðŸ‘¤ Username: admin"
            echo "ðŸ”‘ Password: (from GRAFANA_PASS secret)"
          else
            echo "â³ LoadBalancer is being created..."
          fi
          echo "============================================"

      - name: Smoke Test and Alert Test
        run: |
          URL="https://www.saharbittman.com/health"
          
          echo "ðŸ§ª Starting smoke test for: $URL"
          echo "â³ Waiting for application to be ready..."
          
          for i in {1..60}; do
            HTTP_CODE=$(curl -s -o /tmp/response.txt -w "%{http_code}" "$URL" 2>/dev/null || echo "000")
            RESPONSE=$(cat /tmp/response.txt 2>/dev/null || echo "")
            
            echo "Attempt $i/60 - HTTP: $HTTP_CODE - Body: $RESPONSE"
            
            if [ "$HTTP_CODE" = "200" ] && (echo "$RESPONSE" | grep -q "OK"); then
              echo "âœ… App is live and healthy!"
              
              echo ""
              echo "ðŸ§ª Testing alert system..."
              kubectl scale deployment flask-app --replicas=1 -n default
              echo "âœ… Scaled to 1 replica"
              echo "ðŸ“§ Alert should fire and email sent to: saharr449@gmail.com"
              
              exit 0
            fi
            
            sleep 30
          done
          
          echo "âŒ Smoke test failed after 60 attempts"
          echo "Last response: $RESPONSE"
          exit 1  
        shell: bash

  # ============================================
  # Job 2: DESTROY (Cleanup)
  # ============================================
  destroy:
    runs-on: ubuntu-latest
    if: github.event_name == 'workflow_dispatch' && github.event.inputs.action == 'destroy'
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Configure AWS
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          aws-region: ${{ env.AWS_REGION }}
          role-session-name: github-actions-destroy

      - name: Update kubeconfig
        run: |
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.CLUSTER_NAME }}

      - name: Cleanup Kubernetes & ArgoCD & AWS Resources
        run: |
          echo "ðŸ—‘ï¸ Deleting ArgoCD ApplicationSets..."
          kubectl delete applicationsets --all -n argocd --timeout=5m || true

          echo "â³ Waiting 10s..."
          sleep 10

          echo "ðŸ—‘ï¸ Deleting ArgoCD Applications..."
          kubectl delete applications --all -n argocd --timeout=5m || true

          echo "â³ Waiting 10s..."
          sleep 10

          echo "ðŸ—‘ï¸ Uninstalling Prometheus stack via Helm..."
          helm uninstall prom-stack -n monitoring --wait --timeout=5m || true

          echo "â³ Waiting 30s for Prometheus cleanup..."
          sleep 30

          # âœ… Delete Ingresses first
          echo "ðŸ—‘ï¸ Deleting all Ingresses..."
          kubectl delete ingress --all --all-namespaces --timeout=5m || true

          echo "â³ Waiting 120s for ALB deletion..."
          sleep 120

          echo "ðŸ—‘ï¸ Uninstalling ArgoCD via Helm..."
          helm uninstall argocd -n argocd --wait --timeout=10m || true

          echo "â³ Waiting 60s for LoadBalancer deletion..."
          sleep 60

          # ... (rest of your cleanup script)

          
          echo "ðŸ—‘ï¸ Cleaning up VPC resources..."
          VPC_ID=$(aws ec2 describe-vpcs --filters "Name=tag:Name,Values=${{ env.VPC_NAME }}" --query "Vpcs[0].VpcId" --output text)
          if [ -n "$VPC_ID" ] && [ "$VPC_ID" != "None" ]; then
            echo "Found VPC: $VPC_ID"
            
            # âœ… NEW - Find ALL Load Balancers in VPC
            echo "ðŸ—‘ï¸ Finding all Load Balancers in VPC..."
            LB_ARNS=$(aws elbv2 describe-load-balancers \
              --query "LoadBalancers[?VpcId=='$VPC_ID'].LoadBalancerArn" \
              --output text)
            
            for LB in $LB_ARNS; do
              echo "ðŸ—‘ï¸ Deleting Load Balancer: $LB"
              LISTENERS=$(aws elbv2 describe-listeners \
                --load-balancer-arn "$LB" \
                --query "Listeners[].ListenerArn" \
                --output text)
              for L in $LISTENERS; do
                aws elbv2 delete-listener --listener-arn "$L" || true
              done
              aws elbv2 delete-load-balancer --load-balancer-arn "$LB" || true
            done
            
            echo "â³ Waiting 120s for Load Balancer deletion..."
            sleep 120
            
            # Delete Target Groups
            echo "ðŸ—‘ï¸ Deleting Target Groups..."
            TG_ARNS=$(aws elbv2 describe-target-groups \
              --query "TargetGroups[?VpcId=='$VPC_ID'].TargetGroupArn" \
              --output text)
            for TG in $TG_ARNS; do
              aws elbv2 delete-target-group --target-group-arn "$TG" || true
            done
            sleep 30

            # âœ… NEW - Delete Network Interfaces BEFORE Security Groups
            echo "ðŸ—‘ï¸ Deleting Network Interfaces..."
            ENI_IDS=$(aws ec2 describe-network-interfaces \
              --filters "Name=vpc-id,Values=$VPC_ID" \
              --query "NetworkInterfaces[?Status=='available'].NetworkInterfaceId" \
              --output text)
            for ENI in $ENI_IDS; do
              echo "ðŸ—‘ï¸ Deleting ENI: $ENI"
              aws ec2 delete-network-interface --network-interface-id "$ENI" || true
            done
            sleep 30

            # Delete Security Groups
            echo "ðŸ—‘ï¸ Deleting Security Groups..."
            SG_IDS=$(aws ec2 describe-security-groups \
              --filters "Name=vpc-id,Values=$VPC_ID" \
              --query "SecurityGroups[?GroupName!='default'].GroupId" \
              --output text)
            
            # First pass - remove all rules
            for SG in $SG_IDS; do
              echo "ðŸ—‘ï¸ Removing rules from SG: $SG"
              INGRESS=$(aws ec2 describe-security-groups \
                --group-ids "$SG" \
                --query 'SecurityGroups[0].IpPermissions' \
                --output json || echo "[]")
              EGRESS=$(aws ec2 describe-security-groups \
                --group-ids "$SG" \
                --query 'SecurityGroups[0].IpPermissionsEgress' \
                --output json || echo "[]")
              
              [ "$INGRESS" != "[]" ] && \
                aws ec2 revoke-security-group-ingress \
                  --group-id "$SG" \
                  --ip-permissions "$INGRESS" 2>/dev/null || true
              [ "$EGRESS" != "[]" ] && \
                aws ec2 revoke-security-group-egress \
                  --group-id "$SG" \
                  --ip-permissions "$EGRESS" 2>/dev/null || true
            done
            
            sleep 10
            
            # Second pass - delete security groups
            for SG in $SG_IDS; do
              echo "ðŸ—‘ï¸ Deleting SG: $SG"
              aws ec2 delete-security-group --group-id "$SG" 2>/dev/null || true
            done
            
            echo "âœ… VPC resources cleanup completed"
          fi

  # ============================================
  # Job 3: DESTROY TERRAFORM (Bootstrap)
  # ============================================
  destroy-bootstrap:
    needs: destroy
    runs-on: ubuntu-latest
    if: needs.destroy.result == 'success'
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: 1.5.7

      - name: Configure AWS
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          aws-region: ${{ env.AWS_REGION }}
          role-session-name: github-actions-terraform-destroy

      - name: Terraform Init
        run: |
          cd infra
          terraform init -upgrade

      - name: Destroy Bootstrap Infrastructure
        run: |
          cd infra
          terraform destroy -auto-approve
          
      - name: Cleanup Complete
        run: |
          echo "ðŸŽ‰ ================================"
          echo "âœ… Infrastructure destroyed successfully!"
          echo "================================"