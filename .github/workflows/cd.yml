name: CD to EKS

on:
  workflow_run:
    workflows: ["CI Build & Test"]
    types:
      - completed
  workflow_dispatch:
    inputs:
      action:
        description: "Apply or Destroy"
        required: true
        default: "apply"
        type: choice
        options:
          - apply
          - destroy

permissions:
  id-token: write
  contents: read

env:
  AWS_REGION: us-west-2
  CLUSTER_NAME: eksdemo-cluster
  VPC_NAME: eksdemo-vpc
  ARGOCD_CLB: eksdemo-argocd-clb
  FLASK_ALB: eksdemo-flask-alb
  DOMAIN: saharbittman.net

jobs:
  deploy:
    runs-on: ubuntu-latest
    if: >
      ${{
        github.event_name == 'workflow_dispatch' ||
        (
          github.event_name == 'workflow_run' &&
          github.event.workflow_run.name == 'CI Build & Test' &&
          github.event.workflow_run.conclusion == 'success'
        )
      }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Configure AWS
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          aws-region: ${{ env.AWS_REGION }}
          role-session-name: github-actions

      # ========================================
      # üîç Trivy Security Scan - ArgoCD Manifests Only
      # ========================================
      
      - name: üîç Trivy - Scan ArgoCD Kubernetes Manifests
        if: ${{ github.event_name == 'workflow_dispatch' && github.event.inputs.action == 'apply' || github.event_name == 'workflow_run' }}
        continue-on-error: true
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: 'config'
          scan-ref: 'ArgoCD/'
          format: 'table'
          exit-code: '0'
          severity: 'CRITICAL,HIGH'
          trivyignores: '.trivyignore'

      # Update kubeconfig
      - name: Update kubeconfig for EKS
        if: ${{ github.event_name == 'workflow_dispatch' && github.event.inputs.action == 'apply' || github.event_name == 'workflow_run' }}
        run: |
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.CLUSTER_NAME }}

      # Configure aws-auth
      - name: Configure aws-auth
        if: ${{ github.event_name == 'workflow_dispatch' && github.event.inputs.action == 'apply' || github.event_name == 'workflow_run' }}
        run: |
          cat <<EOF | kubectl apply -f -
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: aws-auth
            namespace: kube-system
          data:
            mapUsers: |
              - userarn: arn:aws:iam::${{ secrets.AWS_ACCOUNT_NUMBER }}:user/${{ secrets.AWS_USER }}
                username: ${{ secrets.AWS_USER }}
                groups:
                  - system:masters
          EOF

      # Install ArgoCD
      - name: Install ArgoCD
        if: ${{ github.event_name == 'workflow_dispatch' && github.event.inputs.action == 'apply' || github.event_name == 'workflow_run' }}
        run: |
          helm repo add argo https://argoproj.github.io/argo-helm
          helm repo update
          helm upgrade --install argocd argo/argo-cd \
            --namespace argocd \
            --create-namespace \
            --set server.service.type=LoadBalancer \
            --set-string server.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-type"=classic \
            --set server.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-name"="${{ env.ARGOCD_CLB }}"

          echo "‚è≥ Waiting for argocd-server rollout..."
          kubectl -n argocd rollout status deployment/argocd-server --timeout=5m

          # Get the LoadBalancer DNS
          ARGOCD_LB_DNS=$(kubectl -n argocd get svc argocd-server -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
          echo "üåê ArgoCD LoadBalancer DNS: $ARGOCD_LB_DNS"

          # Get initial admin password
          ARGOCD_ADMIN_PASSWORD=$(kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 --decode)
          echo "üîë ArgoCD initial admin password: $ARGOCD_ADMIN_PASSWORD"


       # Deploy apps
      - name: Get VPC and Deploy Apps
        if: ${{ github.event_name == 'workflow_dispatch' && github.event.inputs.action == 'apply' || github.event_name == 'workflow_run' }}
        run: |
          export VPC_ID=$(aws ec2 describe-vpcs \
            --filters "Name=tag:Name,Values=${{ env.VPC_NAME }}" \
            --query "Vpcs[0].VpcId" --output text)
          export ECR_NAME=${{ secrets.ECR_NAME }}
          export AWS_ACCOUNT_NUMBER=${{ secrets.AWS_ACCOUNT_NUMBER }}
          envsubst < ArgoCD/externalCharts/alb_dns.yaml | kubectl apply -f -
          envsubst < ArgoCD/externalCharts/secrets.yaml | kubectl apply -f -
          envsubst < ArgoCD/myChart/argo.yaml | kubectl apply -f -
          sleep 10
          kubectl get applications -n argocd

      # Smoke test
      - name: Smoke Test
        if: ${{ github.event_name == 'workflow_dispatch' && github.event.inputs.action == 'apply' || github.event_name == 'workflow_run' }}
        run: |
          URL="https://www.saharbittman.com/health"

          for i in {1..200}; do
            response=$(curl -k -s "$URL" || true)

            if [ "${response,,}" = "ok" ]; then
              echo "‚úÖ Health check passed! Response: $response"
              exit 0
            fi

            echo "‚è≥ Attempt $i/50 - Response: $response"
            sleep 10
          done

          echo "‚ùå Smoke test failed - health endpoint did not return 'ok' after 50 attempts"
          exit 1



      # # Push image to second AWS account only if CD succeeded

      # - name: Start ECR replicate to second account
      #   if: ${{ success() }}
      #   run: echo "üöÄ Starting image replication to second ECR..."

      # # Login to SOURCE ECR (first account)
      # - name: Login to SOURCE ECR
      #   if: ${{ success() }}
      #   uses: aws-actions/amazon-ecr-login@v2

      # # Pull image from SOURCE ECR
      # - name: Pull image from SOURCE ECR
      #   if: ${{ success() }}
      #   env:
      #     SRC_ECR: ${{ secrets.ECR_NAME }}
      #     TAG: latest
      #   run: |
      #     echo "üì• Pulling image: $SRC_ECR:$TAG"
      #     docker pull "$SRC_ECR:$TAG"

      # # Assume role in SECOND ACCOUNT
      # - name: Configure AWS (SECOND ACCOUNT)
      #   if: ${{ success() }}
      #   uses: aws-actions/configure-aws-credentials@v4
      #   with:
      #     role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME_SECOND_ACCOUNT }}
      #     aws-region: ${{ env.AWS_REGION }}
      #     role-session-name: github-actions-repl-second

      # # Login to SECOND ACCOUNT ECR
      # - name: Login to SECOND ACCOUNT ECR
      #   if: ${{ success() }}
      #   uses: aws-actions/amazon-ecr-login@v2

      # # Tag and Push to SECOND ACCOUNT ECR
      # - name: Tag & Push to SECOND ACCOUNT ECR
      #   if: ${{ success() }}
      #   env:
      #     SRC_ECR: ${{ secrets.ECR_NAME }}
      #     DST_ECR: ${{ secrets.ECR_NAME_SECOND }}
      #     TAG: latest
      #   run: |
      #     echo "üè∑ Tagging: $SRC_ECR:$TAG ‚Üí $DST_ECR:$TAG"
      #     docker tag "$SRC_ECR:$TAG" "$DST_ECR:$TAG"

      #     echo "üì§ Pushing image to SECOND ACCOUNT:"
      #     docker push "$DST_ECR:$TAG"

      #     echo "‚úÖ Replication done!"


       # ========== DESTROY ==========
      - name: Destroy Everything
        if: github.event.inputs.action == 'destroy' || failure()
        run: |
          
             # ========== DESTROY ==========
      - name: Destroy Everything
        if: github.event.inputs.action == 'destroy' || failure()
        run: |
          VPC_NAME="eksdemo-vpc"

          echo "üîç Getting VPC ID for: $VPC_NAME ..."
          VPC_ID=$(aws ec2 describe-vpcs \
            --filters "Name=tag:Name,Values=$VPC_NAME" \
            --query "Vpcs[0].VpcId" \
            --output text)

          if [ "$VPC_ID" = "None" ] || [ -z "$VPC_ID" ]; then
            echo "‚ùå VPC not found!"
            exit 1
          fi

          echo "‚úÖ VPC found: $VPC_ID"
          echo ""

          # ============================
          # DELETE LOAD BALANCERS (ALB + NLB)
          # ============================
          echo "üßπ Deleting ALBs & NLBs in VPC..."

          LB_ARNS=$(aws elbv2 describe-load-balancers \
            --query "LoadBalancers[?VpcId=='$VPC_ID'].LoadBalancerArn" \
            --output text)

          if [ -n "$LB_ARNS" ]; then
            for LB_ARN in $LB_ARNS; do
              echo "  Deleting LB: $LB_ARN"
              aws elbv2 delete-load-balancer --load-balancer-arn "$LB_ARN"
            done
          else
            echo "  No ALBs/NLBs found."
          fi

          echo "‚è≥ Waiting 40s for LB deletion..."
          sleep 40
          echo ""

          # ============================
          # DELETE TARGET GROUPS
          # ============================
          echo "üßπ Deleting Target Groups in VPC..."

          TG_ARNS=$(aws elbv2 describe-target-groups \
            --query "TargetGroups[?VpcId=='$VPC_ID'].TargetGroupArn" \
            --output text)

          if [ -n "$TG_ARNS" ]; then
            for TG_ARN in $TG_ARNS; do
              echo "  Deleting Target Group: $TG_ARN"
              aws elbv2 delete-target-group --target-group-arn "$TG_ARN"
            done
          else
            echo "  No Target Groups found."
          fi
          echo ""

          # ============================
          # DELETE SECURITY GROUPS
          # ============================
          echo "üßπ Deleting Security Groups in VPC..."

          SG_IDS=$(aws ec2 describe-security-groups \
            --filters "Name=vpc-id,Values=$VPC_ID" \
            --query "SecurityGroups[?GroupName!='default'].GroupId" \
            --output text)

          if [ -n "$SG_IDS" ]; then
            for SG_ID in $SG_IDS; do
              echo "  Deleting Security Group: $SG_ID"
              aws ec2 delete-security-group --group-id "$SG_ID" 2>/dev/null || echo "    ‚ö†Ô∏è Failed (in use)"
            done
          else
            echo "  No Security Groups found."
          fi
          echo ""

          echo "üéâ Cleanup completed for VPC: $VPC_ID"
          

  destroy-bootstrap:
    needs: deploy
    if: github.event.inputs.action == 'destroy' && success()
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: 1.5.7

      - name: Configure AWS
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          aws-region: ${{ env.AWS_REGION }}
          role-session-name: github-actions

      - name: Terraform Init
        run: |
          cd infra
          terraform init -upgrade

      - name: Destroy Bootstrap Infra
        run: |
          cd infra
          terraform destroy -auto-approve