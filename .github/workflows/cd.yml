name: EKS Infrastructure Management

on:
  workflow_run:
    workflows: ["CI Build & Test"]
    types:
      - completed
  workflow_dispatch:
    inputs:
      action:
        description: "Choose action"
        required: true
        type: choice
        options:
          - apply
          - destroy

permissions:
  id-token: write
  contents: read

env:
  AWS_REGION: us-west-2
  CLUSTER_NAME: eksdemo-cluster
  VPC_NAME: eksdemo-vpc
  ARGOCD_CLB: eksdemo-argocd-clb
  FLASK_ALB: eksdemo-flask-alb

jobs:
  # ============================================
  # Job 1: APPLY (Deploy)
  # ============================================
  apply:
    runs-on: ubuntu-latest
    if: |
      (github.event_name == 'workflow_dispatch' && github.event.inputs.action == 'apply') ||
      (github.event_name == 'workflow_run' && github.event.workflow_run.conclusion == 'success')
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Configure AWS
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          aws-region: ${{ env.AWS_REGION }}
          role-session-name: github-actions-apply

      - name: Update kubeconfig for EKS
        run: |
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.CLUSTER_NAME }}
      
      - name: Install Prometheus Operator CRDs
        run: |
          echo "üì¶ Installing Prometheus Operator CRDs..."

          kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.77.2/example/prometheus-operator-crd/monitoring.coreos.com_alertmanagerconfigs.yaml
          kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.77.2/example/prometheus-operator-crd/monitoring.coreos.com_alertmanagers.yaml
          kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.77.2/example/prometheus-operator-crd/monitoring.coreos.com_podmonitors.yaml
          kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.77.2/example/prometheus-operator-crd/monitoring.coreos.com_probes.yaml
          kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.77.2/example/prometheus-operator-crd/monitoring.coreos.com_prometheuses.yaml
          kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.77.2/example/prometheus-operator-crd/monitoring.coreos.com_prometheusrules.yaml
          kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.77.2/example/prometheus-operator-crd/monitoring.coreos.com_scrapeconfigs.yaml
          kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.77.2/example/prometheus-operator-crd/monitoring.coreos.com_servicemonitors.yaml
          kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.77.2/example/prometheus-operator-crd/monitoring.coreos.com_thanosrulers.yaml

          echo "‚è≥ Waiting for CRDs to be established..."

          kubectl wait --for=condition=established crd/prometheuses.monitoring.coreos.com --timeout=120s
          kubectl wait --for=condition=established crd/alertmanagers.monitoring.coreos.com --timeout=120s
          kubectl wait --for=condition=established crd/prometheusrules.monitoring.coreos.com --timeout=120s

          echo "‚úÖ Prometheus CRDs are ready"


      - name: Configure aws-auth
        run: |
          cat <<EOF | kubectl apply -f -
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: aws-auth
            namespace: kube-system
          data:
            mapUsers: |
              - userarn: arn:aws:iam::${{ secrets.AWS_ACCOUNT_NUMBER }}:user/${{ secrets.AWS_USER }}
                username: ${{ secrets.AWS_USER }}
                groups:
                  - system:masters
          EOF

      - name: Install ArgoCD
        run: |
          helm repo add argo https://argoproj.github.io/argo-helm
          helm repo update
          helm upgrade --install argocd argo/argo-cd \
            --namespace argocd \
            --create-namespace \
            --set server.service.type=LoadBalancer \
            --set-string server.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-type"=classic \
            --set server.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-name"="${{ env.ARGOCD_CLB }}"

          echo "‚è≥ Waiting for argocd-server rollout..."
          kubectl -n argocd rollout status deployment/argocd-server --timeout=5m

          ARGOCD_LB_DNS=$(kubectl -n argocd get svc argocd-server -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
          echo "üåê ArgoCD LoadBalancer DNS: $ARGOCD_LB_DNS"

          ARGOCD_ADMIN_PASSWORD=$(kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 --decode)
          echo "üîë ArgoCD initial admin password: $ARGOCD_ADMIN_PASSWORD"

      - name: Get VPCID, Secrets and Deploy Apps
        run: |
          export VPC_ID=$(aws ec2 describe-vpcs \
            --filters "Name=tag:Name,Values=${{ env.VPC_NAME }}" \
            --query "Vpcs[0].VpcId" --output text)
          export ECR_NAME=${{ secrets.ECR_NAME }}
          export AWS_ACCOUNT_NUMBER=${{ secrets.AWS_ACCOUNT_NUMBER }}
          export GOOGLE_SECRET_PROM=${{ secrets.GOOGLE_SECRET_PROM }}
          export GRAFANA_PASS=${{ secrets.GRAFANA_PASS }}
          envsubst < ArgoCD/externalCharts/alb_dns.yaml | kubectl apply -f -
          envsubst < ArgoCD/externalCharts/secrets.yaml | kubectl apply -f -
          envsubst < ArgoCD/myChart/argo.yaml | kubectl apply -f -
          envsubst < ArgoCD/monitoring/argo.yaml | kubectl apply -f -
          sleep 10
          kubectl get applications -n argocd

      - name: Smoke Test
        run: |
          URL="https://www.saharbittman.com/health"
          
          echo "üß™ Starting smoke test for: $URL"
          echo "‚è≥ Waiting for application to be ready..."
          
          for i in {1..60}; do
            HTTP_CODE=$(curl -s -o /tmp/response.txt -w "%{http_code}" "$URL" 2>/dev/null || echo "000")
            RESPONSE=$(cat /tmp/response.txt 2>/dev/null || echo "")
            
            echo "Attempt $i/60 - HTTP: $HTTP_CODE - Body: $RESPONSE"
            
            # ‚úÖ ◊ë◊ì◊ô◊ß◊î ◊™◊ß◊ô◊†◊î: ◊ê◊ï 200 ◊¢◊ù JSON ◊©◊û◊õ◊ô◊ú "OK" ◊ê◊ï plain text "OK"
            if [ "$HTTP_CODE" = "200" ] && (echo "$RESPONSE" | grep -q "OK"); then
              echo "‚úÖ App is live and healthy!"
              exit 0
            fi
            
            sleep 30
          done
          
          echo "‚ùå Smoke test failed after 60 attempts (30 minutes)"
          echo "Last response: $RESPONSE"
          exit 1  # ‚≠ê ◊™◊ô◊ß◊†◊™◊ô - ◊ó◊°◊® 1
        shell: bash

  # ============================================
  # Job 2: DESTROY (Cleanup)
  # ============================================
  destroy:
    runs-on: ubuntu-latest
    if: github.event_name == 'workflow_dispatch' && github.event.inputs.action == 'destroy'
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Configure AWS
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          aws-region: ${{ env.AWS_REGION }}
          role-session-name: github-actions-destroy

      - name: Update kubeconfig
        run: |
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.CLUSTER_NAME }}

      - name: Cleanup Kubernetes & ArgoCD & AWS Resources
        run: |
          echo "üóëÔ∏è Deleting ArgoCD ApplicationSets..."
          kubectl delete applicationsets --all -n argocd --timeout=5m || true
          
          echo "‚è≥ Waiting 10s..."
          sleep 10
          
          echo "üóëÔ∏è Deleting ArgoCD Applications..."
          kubectl delete applications --all -n argocd --timeout=5m || true
          
          echo "‚è≥ Waiting 30s for applications cleanup..."
          sleep 30
          
          # ‚úÖ NEW - Delete Ingresses first!
          echo "üóëÔ∏è Deleting all Ingresses..."
          kubectl delete ingress --all --all-namespaces --timeout=5m || true
          
          echo "‚è≥ Waiting 120s for ALB deletion..."
          sleep 120
          
          echo "üóëÔ∏è Uninstalling ArgoCD via Helm..."
          helm uninstall argocd -n argocd --wait --timeout=10m || true
          
          echo "‚è≥ Waiting 60s for LoadBalancer deletion..."
          sleep 60

          # ... (rest of your cleanup script)
          
          echo "üóëÔ∏è Cleaning up VPC resources..."
          VPC_ID=$(aws ec2 describe-vpcs --filters "Name=tag:Name,Values=${{ env.VPC_NAME }}" --query "Vpcs[0].VpcId" --output text)
          if [ -n "$VPC_ID" ] && [ "$VPC_ID" != "None" ]; then
            echo "Found VPC: $VPC_ID"
            
            # ‚úÖ NEW - Find ALL Load Balancers in VPC
            echo "üóëÔ∏è Finding all Load Balancers in VPC..."
            LB_ARNS=$(aws elbv2 describe-load-balancers \
              --query "LoadBalancers[?VpcId=='$VPC_ID'].LoadBalancerArn" \
              --output text)
            
            for LB in $LB_ARNS; do
              echo "üóëÔ∏è Deleting Load Balancer: $LB"
              LISTENERS=$(aws elbv2 describe-listeners \
                --load-balancer-arn "$LB" \
                --query "Listeners[].ListenerArn" \
                --output text)
              for L in $LISTENERS; do
                aws elbv2 delete-listener --listener-arn "$L" || true
              done
              aws elbv2 delete-load-balancer --load-balancer-arn "$LB" || true
            done
            
            echo "‚è≥ Waiting 120s for Load Balancer deletion..."
            sleep 120
            
            # Delete Target Groups
            echo "üóëÔ∏è Deleting Target Groups..."
            TG_ARNS=$(aws elbv2 describe-target-groups \
              --query "TargetGroups[?VpcId=='$VPC_ID'].TargetGroupArn" \
              --output text)
            for TG in $TG_ARNS; do
              aws elbv2 delete-target-group --target-group-arn "$TG" || true
            done
            sleep 30

            # ‚úÖ NEW - Delete Network Interfaces BEFORE Security Groups
            echo "üóëÔ∏è Deleting Network Interfaces..."
            ENI_IDS=$(aws ec2 describe-network-interfaces \
              --filters "Name=vpc-id,Values=$VPC_ID" \
              --query "NetworkInterfaces[?Status=='available'].NetworkInterfaceId" \
              --output text)
            for ENI in $ENI_IDS; do
              echo "üóëÔ∏è Deleting ENI: $ENI"
              aws ec2 delete-network-interface --network-interface-id "$ENI" || true
            done
            sleep 30

            # Delete Security Groups
            echo "üóëÔ∏è Deleting Security Groups..."
            SG_IDS=$(aws ec2 describe-security-groups \
              --filters "Name=vpc-id,Values=$VPC_ID" \
              --query "SecurityGroups[?GroupName!='default'].GroupId" \
              --output text)
            
            # First pass - remove all rules
            for SG in $SG_IDS; do
              echo "üóëÔ∏è Removing rules from SG: $SG"
              INGRESS=$(aws ec2 describe-security-groups \
                --group-ids "$SG" \
                --query 'SecurityGroups[0].IpPermissions' \
                --output json || echo "[]")
              EGRESS=$(aws ec2 describe-security-groups \
                --group-ids "$SG" \
                --query 'SecurityGroups[0].IpPermissionsEgress' \
                --output json || echo "[]")
              
              [ "$INGRESS" != "[]" ] && \
                aws ec2 revoke-security-group-ingress \
                  --group-id "$SG" \
                  --ip-permissions "$INGRESS" 2>/dev/null || true
              [ "$EGRESS" != "[]" ] && \
                aws ec2 revoke-security-group-egress \
                  --group-id "$SG" \
                  --ip-permissions "$EGRESS" 2>/dev/null || true
            done
            
            sleep 10
            
            # Second pass - delete security groups
            for SG in $SG_IDS; do
              echo "üóëÔ∏è Deleting SG: $SG"
              aws ec2 delete-security-group --group-id "$SG" 2>/dev/null || true
            done
            
            echo "‚úÖ VPC resources cleanup completed"
          fi

  # ============================================
  # Job 3: DESTROY TERRAFORM (Bootstrap)
  # ============================================
  destroy-bootstrap:
    needs: destroy
    runs-on: ubuntu-latest
    if: needs.destroy.result == 'success'
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: 1.5.7

      - name: Configure AWS
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          aws-region: ${{ env.AWS_REGION }}
          role-session-name: github-actions-terraform-destroy

      - name: Terraform Init
        run: |
          cd infra
          terraform init -upgrade

      - name: Destroy Bootstrap Infrastructure
        run: |
          cd infra
          terraform destroy -auto-approve
          
      - name: Cleanup Complete
        run: |
          echo "üéâ ================================"
          echo "‚úÖ Infrastructure destroyed successfully!"
          echo "================================"