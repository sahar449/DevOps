name: CD to EKS

on:
  workflow_run:
    workflows: ["CI Build & Test"]
    types:
      - completed
  workflow_dispatch:
    inputs:
      action:
        description: "Apply or Destroy"
        required: true
        default: "apply"
        type: choice
        options:
          - apply
          - destroy

permissions:
  id-token: write
  contents: read

env:
  AWS_REGION: us-west-2
  CLUSTER_NAME: eksdemo-cluster
  VPC_NAME: eksdemo-vpc
  ARGOCD_CLB: eksdemo-argocd-clb
  FLASK_ALB: eksdemo-flask-alb
  DOMAIN: saharbittman.com

jobs:
  deploy:
    runs-on: ubuntu-latest
    if: >
      ${{
        (github.event_name == 'workflow_dispatch' && github.event.inputs.action == 'apply') ||
        (
          github.event_name == 'workflow_run' &&
          github.event.workflow_run.conclusion == 'success'
        )
      }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Configure AWS
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          aws-region: ${{ env.AWS_REGION }}
          role-session-name: github-actions

      - name: üîç Trivy - Scan ArgoCD Kubernetes Manifests
        continue-on-error: true
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: 'config'
          scan-ref: 'ArgoCD/'
          format: 'table'
          exit-code: '0'
          severity: 'CRITICAL,HIGH'
          trivyignores: '.trivyignore'

      - name: Update kubeconfig for EKS
        run: |
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.CLUSTER_NAME }}

      - name: Configure aws-auth
        run: |
          cat <<EOF | kubectl apply -f -
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: aws-auth
            namespace: kube-system
          data:
            mapUsers: |
              - userarn: arn:aws:iam::${{ secrets.AWS_ACCOUNT_NUMBER }}:user/${{ secrets.AWS_USER }}
                username: ${{ secrets.AWS_USER }}
                groups:
                  - system:masters
          EOF

      - name: Install ArgoCD
        run: |
          helm repo add argo https://argoproj.github.io/argo-helm
          helm repo update
          helm upgrade --install argocd argo/argo-cd \
            --namespace argocd \
            --create-namespace \
            --set server.service.type=LoadBalancer \
            --set-string server.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-type"=classic \
            --set server.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-name"="${{ env.ARGOCD_CLB }}"

          echo "‚è≥ Waiting for argocd-server rollout..."
          kubectl -n argocd rollout status deployment/argocd-server --timeout=5m

          ARGOCD_LB_DNS=$(kubectl -n argocd get svc argocd-server -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
          echo "üåê ArgoCD LoadBalancer DNS: $ARGOCD_LB_DNS"

          ARGOCD_ADMIN_PASSWORD=$(kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 --decode)
          echo "üîë ArgoCD initial admin password: $ARGOCD_ADMIN_PASSWORD"

      - name: Get VPC and Deploy Apps
        run: |
          export VPC_ID=$(aws ec2 describe-vpcs \
            --filters "Name=tag:Name,Values=${{ env.VPC_NAME }}" \
            --query "Vpcs[0].VpcId" --output text)
          export ECR_NAME=${{ secrets.ECR_NAME }}
          export AWS_ACCOUNT_NUMBER=${{ secrets.AWS_ACCOUNT_NUMBER }}
          envsubst < ArgoCD/externalCharts/alb_dns.yaml | kubectl apply -f -
          envsubst < ArgoCD/externalCharts/secrets.yaml | kubectl apply -f -
          envsubst < ArgoCD/myChart/argo.yaml | kubectl apply -f -
          sleep 10
          kubectl get applications -n argocd

      - name: Smoke Test
        run: |
          URL="https://www.saharbittman.com/health"

          for i in {1..200}; do
            response=$(curl -k -s "$URL" || true)

            if [ "${response,,}" = "ok" ]; then
              echo "‚úÖ Health check passed! Response: $response"
              exit 0
            fi

            echo "‚è≥ Attempt $i/200 - Response: $response"
            sleep 10
          done

          echo "‚ùå Smoke test failed - health endpoint did not return 'ok' after 200 attempts"
          exit 1

  destroy:
    needs: [deploy]
    runs-on: ubuntu-latest
    if: always() && needs.deploy.result != 'success'
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Configure AWS
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          aws-region: ${{ env.AWS_REGION }}
          role-session-name: github-actions

      - name: Update kubeconfig
        run: |
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.CLUSTER_NAME }}

      - name: Cleanup ExternalDNS Records
        run: |
          echo "üßπ Cleaning up specific DNS records..."
          
          DOMAIN="${{ env.DOMAIN }}"
          
          HOSTED_ZONE_ID=$(aws route53 list-hosted-zones \
            --query "HostedZones[?Name=='${DOMAIN}.'].Id" \
            --output text | cut -d'/' -f3)
          
          if [ -z "$HOSTED_ZONE_ID" ]; then
            echo "‚ö†Ô∏è Hosted Zone not found for domain: ${DOMAIN}"
            exit 0
          fi
          
          echo "‚úÖ Found Hosted Zone: $HOSTED_ZONE_ID for domain: ${DOMAIN}"
          
          SUBDOMAINS_TO_DELETE=(
            "www.${DOMAIN}."
            "argocd.${DOMAIN}."
            "*.${DOMAIN}."
          )
          
          for SUBDOMAIN in "${SUBDOMAINS_TO_DELETE[@]}"; do
            echo "üîç Checking: $SUBDOMAIN"
            
            RECORDS=$(aws route53 list-resource-record-sets \
              --hosted-zone-id "$HOSTED_ZONE_ID" \
              --query "ResourceRecordSets[?Name=='${SUBDOMAIN}']" \
              --output json)
            
            if [ "$(echo "$RECORDS" | jq '. | length')" -eq 0 ]; then
              echo "  ‚û§ No records found, skipping..."
              continue
            fi
            
            echo "$RECORDS" | jq -c '.[]' | while read -r record; do
              RECORD_TYPE=$(echo "$record" | jq -r '.Type')
              
              echo "  ‚û§ Deleting $SUBDOMAIN ($RECORD_TYPE)..."
              
              CHANGE_BATCH=$(cat <<EOF
          {
            "Changes": [{
              "Action": "DELETE",
              "ResourceRecordSet": $record
            }]
          }
          EOF
          )
              
              aws route53 change-resource-record-sets \
                --hosted-zone-id "$HOSTED_ZONE_ID" \
                --change-batch "$CHANGE_BATCH" \
                && echo "    ‚úÖ Deleted" \
                || echo "    ‚ö†Ô∏è Failed"
            done
          done
          
          echo "üéâ DNS cleanup completed!"

      - name: Destroy Everything
        run: |
          echo "üóëÔ∏è Deleting ArgoCD ApplicationSets..."
          kubectl delete applicationsets --all -n argocd --timeout=5m || true
          
          echo "‚è≥ Waiting 10s..."
          sleep 10
          
          echo "üóëÔ∏è Deleting ArgoCD Applications..."
          kubectl delete applications --all -n argocd --timeout=5m || true
          
          echo "‚è≥ Waiting 30s for applications cleanup..."
          sleep 30
          
          echo "üóëÔ∏è Uninstalling ArgoCD via Helm..."
          helm uninstall argocd -n argocd --wait --timeout=10m || true
          
          echo "‚è≥ Waiting 60s for LoadBalancer deletion..."
          sleep 60
          
          echo "üîç Verifying ArgoCD CLB deletion..."
          
          CLB_ARN=$(aws elbv2 describe-load-balancers \
            --query "LoadBalancers[?LoadBalancerName=='${{ env.ARGOCD_CLB }}'].LoadBalancerArn" \
            --output text)

          if [ -z "$CLB_ARN" ] || [ "$CLB_ARN" = "None" ]; then
            echo "‚úÖ ArgoCD CLB successfully deleted"
          else
            echo "‚ö†Ô∏è Found orphaned CLB: $CLB_ARN - cleaning up..."
            
            LISTENERS=$(aws elbv2 describe-listeners \
              --load-balancer-arn "$CLB_ARN" \
              --query "Listeners[].ListenerArn" \
              --output text)

            for L in $LISTENERS; do
              echo "  ‚û§ Deleting Listener: $L"
              aws elbv2 delete-listener --listener-arn "$L" || true
            done

            sleep 5
            echo "  ‚û§ Deleting orphaned CLB..."
            aws elbv2 delete-load-balancer --load-balancer-arn "$CLB_ARN" || true
            sleep 40
          fi

          echo "üîç Checking for orphaned ArgoCD Target Groups..."
          
          TG_ARNS=$(aws elbv2 describe-target-groups \
            --query "TargetGroups[?contains(TargetGroupName, 'argocd')].TargetGroupArn" \
            --output text)

          if [ -z "$TG_ARNS" ]; then
            echo "‚úÖ No orphaned Target Groups found"
          else
            echo "‚ö†Ô∏è Cleaning orphaned Target Groups..."
            for TG in $TG_ARNS; do
              echo "  ‚û§ Deleting Target Group: $TG"
              aws elbv2 delete-target-group --target-group-arn "$TG" || true
            done
          fi

          echo "üéâ ArgoCD cleanup completed!"
          
          echo "üóëÔ∏è Deleting argocd namespace..."
          kubectl delete namespace argocd --timeout=5m || true
          
          echo "‚è≥ Waiting 20s..."
          sleep 20

          echo "üîç Getting VPC ID for: ${{ env.VPC_NAME }} ..."
          VPC_ID=$(aws ec2 describe-vpcs \
            --filters "Name=tag:Name,Values=${{ env.VPC_NAME }}" \
            --query "Vpcs[0].VpcId" \
            --output text)

          if [ "$VPC_ID" = "None" ] || [ -z "$VPC_ID" ]; then
            echo "‚ö†Ô∏è VPC not found - skipping VPC cleanup"
          else
            echo "‚úÖ VPC found: $VPC_ID"

            echo "üßπ Deleting ALB Listeners..."
            LB_ARNS=$(aws elbv2 describe-load-balancers \
              --query "LoadBalancers[?VpcId=='$VPC_ID'].LoadBalancerArn" \
              --output text)

            for LB in $LB_ARNS; do
              LISTENERS=$(aws elbv2 describe-listeners \
                --load-balancer-arn "$LB" \
                --query "Listeners[].ListenerArn" \
                --output text)

              for L in $LISTENERS; do
                echo "  ‚û§ Deleting Listener: $L"
                aws elbv2 delete-listener --listener-arn "$L" || true
              done
            done

            sleep 5

            echo "üßπ Deleting ALBs..."
            for LB in $LB_ARNS; do
              echo "  ‚û§ Deleting Load Balancer: $LB"
              aws elbv2 delete-load-balancer --load-balancer-arn "$LB" || true
            done

            echo "‚è≥ Waiting 60s for ALB deletion..."
            sleep 60

            echo "üßπ Deleting Target Groups..."
            TG_ARNS=$(aws elbv2 describe-target-groups \
              --query "TargetGroups[?VpcId=='$VPC_ID'].TargetGroupArn" \
              --output text)

            for TG in $TG_ARNS; do
              echo "  ‚û§ Deleting Target Group: $TG"
              aws elbv2 delete-target-group --target-group-arn "$TG" || true
            done

            echo "‚è≥ Waiting 20s..."
            sleep 20

            echo "üîß Cleaning Security Groups..."
            SG_IDS=$(aws ec2 describe-security-groups \
              --filters "Name=vpc-id,Values=$VPC_ID" \
              --query "SecurityGroups[?GroupName!='default'].GroupId" \
              --output text)

            for SG in $SG_IDS; do
              echo "  ‚û§ Checking SG: $SG"
              
              ENIS=$(aws ec2 describe-network-interfaces \
                --filters "Name=group-id,Values=$SG" \
                --query "NetworkInterfaces[].NetworkInterfaceId" \
                --output text)

              for ENI in $ENIS; do
                echo "    - Deleting ENI: $ENI"
                aws ec2 delete-network-interface --network-interface-id "$ENI" || true
              done
            done

            echo "‚è≥ Waiting 20s for ENI cleanup..."
            sleep 20

            echo "üßπ Deleting Security Groups (1st attempt)..."
            for SG in $SG_IDS; do
              SG_NAME=$(aws ec2 describe-security-groups \
                --group-ids "$SG" \
                --query "SecurityGroups[0].GroupName" \
                --output text 2>/dev/null || echo "")
              
              echo "  ‚û§ Deleting SG: $SG ($SG_NAME)"
              aws ec2 delete-security-group --group-id "$SG" 2>/dev/null \
                && echo "    ‚úî Deleted" \
                || echo "    ‚ö†Ô∏è Still in use, will retry..."
            done

            echo "‚è≥ Waiting 30s before retry..."
            sleep 30

            echo "üßπ Deleting Security Groups (2nd attempt)..."
            for SG in $SG_IDS; do
              SG_EXISTS=$(aws ec2 describe-security-groups \
                --group-ids "$SG" \
                --query "SecurityGroups[0].GroupId" \
                --output text 2>/dev/null || echo "")
              
              if [ -n "$SG_EXISTS" ] && [ "$SG_EXISTS" != "None" ]; then
                echo "  ‚û§ Retry deleting SG: $SG"
                aws ec2 delete-security-group --group-id "$SG" 2>/dev/null \
                  && echo "    ‚úî Deleted" \
                  || echo "    ‚ö†Ô∏è Failed - may require manual deletion"
              fi
            done

            echo "‚è≥ Waiting 20s..."
            sleep 20

            echo "üßπ Final Security Groups cleanup (3rd attempt)..."
            for SG in $SG_IDS; do
              SG_EXISTS=$(aws ec2 describe-security-groups \
                --group-ids "$SG" \
                --query "SecurityGroups[0].GroupId" \
                --output text 2>/dev/null || echo "")
              
              if [ -n "$SG_EXISTS" ] && [ "$SG_EXISTS" != "None" ]; then
                echo "  ‚û§ Final attempt for SG: $SG"
                
                aws ec2 revoke-security-group-ingress \
                  --group-id "$SG" \
                  --ip-permissions "$(aws ec2 describe-security-groups \
                    --group-ids "$SG" \
                    --query 'SecurityGroups[0].IpPermissions' \
                    --output json)" 2>/dev/null || true
                
                aws ec2 revoke-security-group-egress \
                  --group-id "$SG" \
                  --ip-permissions "$(aws ec2 describe-security-groups \
                    --group-ids "$SG" \
                    --query 'SecurityGroups[0].IpPermissionsEgress' \
                    --output json)" 2>/dev/null || true
                
                sleep 5
                
                aws ec2 delete-security-group --group-id "$SG" 2>/dev/null \
                  && echo "    ‚úî Deleted" \
                  || echo "    ‚ùå Could not delete - manual intervention required"
              fi
            done
          fi

          echo "üéâ Full cleanup completed!"

  destroy-bootstrap:
    needs: destroy
    if: github.event_name == 'workflow_dispatch' && github.event.inputs.action == 'destroy'
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: 1.5.7

      - name: Configure AWS
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          aws-region: ${{ env.AWS_REGION }}
          role-session-name: github-actions

      - name: Terraform Init
        run: |
          cd infra
          terraform init -upgrade

      - name: Destroy Bootstrap Infra
        run: |
          cd infra
          terraform destroy -auto-approve